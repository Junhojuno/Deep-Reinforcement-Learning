{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Q-Learning의 문제점 : 내가 낸 답이 최선의 선택이 아닐 수 있다.\n",
    "\n",
    "#### 이를 해결하기위해 안가본 곳도 가봐야한다.(더 안좋은 선택일수도 있지만, 도전한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Exploit&Exploration.png\" alt=\"Exploration\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit and Exploration 방법\n",
    "\n",
    "### 1. E-greedy example\n",
    "```python\n",
    "e = 0.1\n",
    "if rand < e:\n",
    "    a(action) = random\n",
    "else:\n",
    "    a = argmax(Q(s,a))\n",
    "```\n",
    "\n",
    "### 2. Decaying E-greedy example\n",
    ": 뒤로 갈수록 영향력이 줄어든다.\n",
    "```python\n",
    "for i in range(num_episodes):\n",
    "    e = 0.1 / ((i / 100) + 1)\n",
    "    while not done:\n",
    "        # choose an action by a greedy\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample() # 상하좌우 random으로 갑니다\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :]) # 가장 값이 높은 곳으로 갑니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add Random Noise\n",
    ": random value를 더해준다. <br>\n",
    "`a = argmax(Q(s,a) + random_values)` or `a = argmax(Q(s,a) + random_values / (i+1)) : decaying 적용`\n",
    "\n",
    "##### example\n",
    ": **그냥 했다면 2번째로 진행했겠지만, random noise를 더해줘서 3번째 선택하는 결과가 나옴.** <br>\n",
    "a = argmax([0.5, 0.6, 0.3, 0.2, 0.5] + [0.1, 0.2, 0.7, 0.3, 0.1]) <br>\n",
    "a = argmax([0.6, 0.8, 1.0, 0.5, 0.6])\n",
    "\n",
    "```python\n",
    "action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 과연 아래의 경우 1번 경로로 가야하나, 2번 경로로 가야하나?\n",
    "<img src=\"../img/Discounted_Reward.PNG\" alt=\"discounted_reward\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 경우 discounted reward를 적용\n",
    ": 나중에 받게 되는 reward를 깎는다. <br>\n",
    "$\\hat{Q}(s,a) = r + \\gamma * \\underset{a^`}{\\operatorname{max}} \\hat{Q}(s^`,a^`)$\n",
    "##### example\n",
    "$\\hat{Q}(s,a) = r + 0.9 * \\underset{a^`}{\\operatorname{max}} \\hat{Q}(s^`,a^`)$\n",
    "\n",
    "#### 다른 형태로 나타내보면....\n",
    "$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} +... \\gamma^{n-t} r_n$ <br>\n",
    "$R_t = r_t + \\gamma R_{t+1}$\n",
    "\n",
    "```python\n",
    "# discount factor\n",
    "dis = 0.99\n",
    "Q[state,action] = reward + dis * np.max(Q[new_state, :])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드\n",
    "\n",
    "```python\n",
    "import gym\n",
    "import gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id = 'FrozenLake-v3',\n",
    "    entry_point = 'gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False}\n",
    ")\n",
    "env = gym.make('FrozenLake-v3')\n",
    "\n",
    "# initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# discount factor\n",
    "dis = 0.99\n",
    "num_episodes = 2000\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "rLIst = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # reset environment and get first new observation\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    \n",
    "    # The Q-Table learning algorithm\n",
    "    while not done:\n",
    "        # choose an action by greedily with noise picking from Q table\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1,env.action_space.n) / (i + 1))\n",
    "        \n",
    "        # Get new state and reward from environment\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update Q-Table with new knowledge using decay rate\n",
    "        Q[state,action] = reward + dis * np.max(Q[new_state, :])\n",
    "        \n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "    rList.append(rAll)\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
