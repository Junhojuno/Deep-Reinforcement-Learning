{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, obs_dim, n_action, seed=0,\n",
    "                 discount_factor = 0.995, epsilon_decay = 0.999, epsilon_min = 0.01,\n",
    "                 learning_rate = 1e-3,\n",
    "                 batch_size = 64,\n",
    "                 memory_size = 2000, hidden_unit_size = 64):\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "        # Environment information\n",
    "        self.obs_dim = obs_dim\n",
    "        self.n_action = n_action\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        # Epsilon Greedy Policy\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Network Hyperparameters\n",
    "        self.hidden_unit_size = hidden_unit_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.train_start = 1000 # ?\n",
    "        \n",
    "        # Experience Replay\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Define Conputational Graph in Tensorflow\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.build_placeholders()\n",
    "            self.build_model()\n",
    "            self.build_loss()\n",
    "            self.build_update_operation()\n",
    "            self.init_session() # Initialize all parameters in graph\n",
    "        \n",
    "        def build_placeholders(self):\n",
    "            # input state\n",
    "            self.obs_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.obs_dim], name='obs')\n",
    "            # TD target\n",
    "            self.target_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.n_action], name='target')\n",
    "            self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='lr')\n",
    "        \n",
    "        def build_model(self):\n",
    "            # build networks\n",
    "            hid1_size = self.hidden_unit_size\n",
    "            hid2_size = self.hidden_unit_size\n",
    "            \n",
    "            with tf.variable_scope(name_or_scope='q_prediction'): # prediction network\n",
    "                # action의 갯수만큼 출력이 나와야한다. (output dimension)\n",
    "                out = tf.layers.dense(inputs=self.obs_ph, units=hid1_size, activation=tf.tanh,\n",
    "                                      kernel_initializer=tf.random_normal_initializer(stddev=0.01, seed=self.seed), name='hidden1')\n",
    "                out = tf.layers.dense(inputs=out, units=hid2_size, activation=tf.tanh,\n",
    "                                      kernel_initializer=tf.random_normal_initializer(stddev=0.01, seed=self.seed), name='hidden2')\n",
    "                self.q_predict = tf.layers.dense(inputs=out, units=self.n_action,\n",
    "                                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01, seed=self.seed), name='q_predict')\n",
    "            \n",
    "            with tf.variable_scope(name_or_scope='q_target'): # target network\n",
    "                # action의 갯수만큼 출력이 나와야한다. (output dimension)\n",
    "                out = tf.layers.dense(inputs=self.obs_ph, units=hid1_size, activation=tf.tanh,\n",
    "                                      kernel_initializer=tf.random_normal_initializer(stddev=0.01, seed=self.seed), name='hidden1')\n",
    "                out = tf.layers.dense(inputs=out, units=hid2_size, activation=tf.tanh,\n",
    "                                      kernel_initializer=tf.random_normal_initializer(stddev=0.01, seed=self.seed), name='hidden2')\n",
    "                self.q_predict_old = tf.layers.dense(inputs=out, units=self.n_action,\n",
    "                                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01, seed=self.seed), name='q_predict')\n",
    "            \n",
    "            # weights 출력 : scope를 설정해놓았기 때문에 해당 scope의 weight를 한꺼번에 출력 가능\n",
    "            # 먼저, prediction network의 parameter를 가져오자\n",
    "            self.weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_prediction')\n",
    "            # 다음으로 target network의 parameter를 가져오자\n",
    "            self.weights_old = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_target')\n",
    "            \n",
    "\n",
    "        def build_loss(self):\n",
    "            self.loss = 0.5*tf.reduce_sum(tf.square(self.target_ph - self.q_predict)) # TD target값은 q_predict_old를 가지고 계산해서 나와야하는거 아닌가?\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph).minimize(self.loss)\n",
    "        \n",
    "        def build_update_operation(self):\n",
    "            # Define parameter update operation in TF graph\n",
    "            update_ops = []\n",
    "            # target network의 parameter를 prediction network의 parameter로 업데이트한다.\n",
    "            for var, var_old in zip(self.weights, self.weights_old):\n",
    "                update_ops.append(var_old.assign(var)) # parameter를 덮어씌운다(assgign).\n",
    "            self.update_ops = update_ops\n",
    "            \n",
    "        def init_session(self):\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True # gpu쓰기위한 코드??\n",
    "            self.sess = tf.Session(config=config, graph=self.g) # session 초기화\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(self.update_ops) # 얘는 feed_dict없어도 되나??\n",
    "            \n",
    "            # summary writer\n",
    "            summary_q = tf.summary.scalar(name='max_Q_predict', tensor=tf.reduce_max(self.q_predict))\n",
    "            summary_q_old = tf.summary.scalar(name='max_Q_target', tensor=tf.reduce_max(self.q_predict_old))\n",
    "            summary_loss = tf.summary.scalar(name='loss', tensor=self.loss)\n",
    "            self.merge_q_step = 0\n",
    "            self.merge_q = tf.summary.merge([summary_q, summary_q_old])\n",
    "            self.merge_loss_step = 0\n",
    "            self.merge_loss = tf.summary.merge([summary_loss])\n",
    "            self.summary_writer = tf.summary.FileWriter('./tf_logs/dqn', graph=self.sess.graph)\n",
    "            \n",
    "        \n",
    "        def update_target(self):\n",
    "            # update parameters\n",
    "            self.sess.run(self.update_ops)\n",
    "            \n",
    "        def update_policy(self):\n",
    "            # update epsilon\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        def get_prediction_old(self, obs):\n",
    "            # target network에서 Q value를 가져온다.\n",
    "            q_value_old, summary = self.sess.run([self.q_predict_old, self.merge_q], feed_dict={self.obs_ph:obs})\n",
    "            \n",
    "            # summary Q value\n",
    "            self.merge_q_step += 1\n",
    "            self.summary_writer.add_summary(summary, self.merge_q_step)\n",
    "            \n",
    "            return q_value_old\n",
    "        \n",
    "        def get_prediction(self, obs):\n",
    "            # prediction network에서 Q value를 가져온다.\n",
    "            q_value, summary = self.sess.run([self.q_predict, self.merge_q], feed_dict={self.obs_ph:obs})\n",
    "            \n",
    "            # summary Q value\n",
    "            self.merge_q_step += 1\n",
    "            self.summary_writer.add_summary(summary, self.merge_q_step)\n",
    "            \n",
    "            return q_value\n",
    "        \n",
    "        def get_action(self, obs):\n",
    "            # epsilon greedy policy\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                return random.randrange(self.n_action)\n",
    "            else:\n",
    "                q_value = self.get_prediction([obs])\n",
    "                return np.argmax(q_value[0]) # 0 indexing은 뭔가?\n",
    "            \n",
    "        def add_experience(self, obs, action, reward, next_obs, done):\n",
    "            # memory에 experience sample 넣기\n",
    "            self.memory.append((obs,action,reward, next_obs, done))\n",
    "            \n",
    "        def train_model(self):\n",
    "            loss = np.nan\n",
    "            n_entries = len(self.memory)\n",
    "            \n",
    "            # experience 갯수가 정해놓은 수보다 커지면 training을 시작한다.\n",
    "            if n_entries > self.train_start:\n",
    "                # random batch sampling\n",
    "                mini_batch = random.sample(self.memory, self.batch_size)\n",
    "                \n",
    "                observations = np.zeros((self.batch_size, self.obs_dim))\n",
    "                next_observations = np.zeros((self.batch_size, self.obs_dim))\n",
    "                actions, rewards, dones = [], [], []\n",
    "                \n",
    "                for i in range(self.batch_size):\n",
    "                    observations[i] = mini_batch[i][0] #obs를 넣어준다.\n",
    "                    actions.append(mini_batch[i][1])\n",
    "                    rewards.append(mini_batch[i][2])\n",
    "                    next_observations[i] = mini_batch[i][3]\n",
    "                    dones.append(mini_batch[i][4])\n",
    "                \n",
    "                target = self.get_prediction(observation) # get_prediction_old가 아니고?\n",
    "                next_q_value = self.get_prediction_old(next_observations)\n",
    "                \n",
    "                # bellman update rule\n",
    "                for i in range(self.batch_size):\n",
    "                    if dones[i]:\n",
    "                        target[i][actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        target[i][action[i]] = rewards[i] + self.discount_factor * (np.max(next_q_value[i]))\n",
    "                    \n",
    "                loss, _, summary = self.sess.run([self.loss, self.optim, self.merge_loss],\n",
    "                                                feed_dict={self.obs_ph:observations, self.target_ph:target, self.learning_rate_ph:self.learning_rate})\n",
    "                \n",
    "                # summary loss\n",
    "                self.merge_loss_step += 1\n",
    "                self.summary_writer.add_summary(summary, self.merge_loss_step)\n",
    "            return loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
