{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junhojuno/Deep-Reinforcement-Learning/blob/master/fastcampus/week4/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4eVSJy-JtRfd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fzfOngrtib4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7028e59d-f251-4d06-f103-719c049bcaf4"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install JSAnimation"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: JSAnimation in /usr/local/lib/python3.6/dist-packages (0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dgkP1ClztRf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper Function"
      ]
    },
    {
      "metadata": {
        "id": "L55miQr3tRf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports specifically so we can render outputs in Jupyter.\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
        "    display(display_animation(anim, default_mode='loop'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vYQbkhg0tRgA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deep Q Learning Agent\n",
        "- target network와 prediction network를 따로 둔다.\n",
        "- replay memory를 두어 transition sampling을 한다."
      ]
    },
    {
      "metadata": {
        "id": "3mvh0NCEtRgC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, obs_dim, n_action, seed=0,\n",
        "                 discount_factor = 0.995, epsilon_decay = 0.999, epsilon_min = 0.01,\n",
        "                 learning_rate = 1e-3, # Step size for Adam\n",
        "                 batch_size = 64, \n",
        "                 memory_size = 2000, hidden_unit_size = 64):\n",
        "        \n",
        "        self.seed = seed \n",
        "        \n",
        "        # Environment Information\n",
        "        self.obs_dim = obs_dim\n",
        "        self.n_action = n_action\n",
        "        self.discount_factor = discount_factor\n",
        "        \n",
        "        # Epsilon Greedy Policy\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        # Network Hyperparameters\n",
        "        self.hidden_unit_size = hidden_unit_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.train_start = 1000\n",
        "\n",
        "        # Experience Replay\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "        \n",
        "        # Define Computational Graph in TF\n",
        "        self.g = tf.Graph()\n",
        "        with self.g.as_default():\n",
        "            self.build_placeholders()\n",
        "            self.build_model()\n",
        "            self.build_loss()\n",
        "            self.build_update_operation()\n",
        "            self.init_session() # Initialize all parameters in graph\n",
        "    \n",
        "    def build_placeholders(self): # Build input and output place holder\n",
        "        self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs') # Input state\n",
        "        self.target_ph = tf.placeholder(tf.float32, (None, self.n_action), 'target') # TD target\n",
        "        self.learning_rate_ph = tf.placeholder(tf.float32, (), 'lr')        \n",
        "    \n",
        "    def build_model(self): # Build networks\n",
        "        hid1_size = self.hidden_unit_size\n",
        "        hid2_size = self.hidden_unit_size\n",
        "        \n",
        "        with tf.variable_scope('q_prediction'): # Prediction Network / Two layered perceptron / Training Parameters\n",
        "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh, # Tangent Hyperbolic Activation\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed=self.seed), name='hidden1')\n",
        "            out = tf.layers.dense(out, hid2_size, tf.tanh, # Tangent Hyperbolic Activation\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed=self.seed), name='hidden2')\n",
        "            self.q_predict = tf.layers.dense(out, self.n_action, # Linear Layer\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed=self.seed), name='q_predict')\n",
        "                        \n",
        "        with tf.variable_scope('q_target'): # Target Network / Two layered perceptron / Old Parameters\n",
        "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh, # Tangent Hyperbolic Activation\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed=self.seed), name='hidden1')\n",
        "            out = tf.layers.dense(out, hid2_size, tf.tanh, # Tangent Hyperbolic Activation\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed=self.seed), name='hidden2')\n",
        "            self.q_predict_old = tf.layers.dense(out, self.n_action, # Linear Layer\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed=self.seed), name='q_predict')\n",
        "        \n",
        "        self.weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_prediction') # Get Prediction network's Parameters\n",
        "        self.weights_old = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_target') # Get Target network's Parameters\n",
        "\n",
        "    def build_loss(self):\n",
        "        # loss는 얼마나 니가 업데이트를 할거냐(이전에 봤던 이동평균)\n",
        "        # for slowly update the target network\n",
        "        # prediction과 true value(target) 사이에 차이가 최소가 되는 지점(optimum)\n",
        "        self.loss = 0.5*tf.reduce_mean(tf.square(self.target_ph - self.q_predict)) # Squared Error\n",
        "        self.optim = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph).minimize(self.loss) # AdamOptimizer (Gradi\n",
        "        \n",
        "    def build_update_operation(self): \n",
        "        # parameter 업데이트\n",
        "        # Define parameter update operation in TF graph\n",
        "        update_ops = [] \n",
        "        for var, var_old in zip(self.weights, self.weights_old): \n",
        "            # Update Target Network's Parameter with Prediction Network\n",
        "            update_ops.append(var_old.assign(var))\n",
        "        self.update_ops = update_ops\n",
        "        \n",
        "    def init_session(self):\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=config,graph=self.g) # Initialize session\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.sess.run(self.update_ops)\n",
        "\n",
        "        # Summary writer\n",
        "        summary_q = tf.summary.scalar('max_Q_predict', tf.reduce_max(self.q_predict))\n",
        "        summary_q_old = tf.summary.scalar('max_Q_target', tf.reduce_max(self.q_predict_old))\n",
        "        summary_loss = tf.summary.scalar('loss', self.loss)\n",
        "        self.merge_q_step = 0\n",
        "        self.merge_q = tf.summary.merge([summary_q, summary_q_old])\n",
        "        self.merge_loss_step = 0\n",
        "        self.merge_loss = tf.summary.merge([summary_loss])\n",
        "        self.summary_writer = tf.summary.FileWriter('./tf_logs/dqn', graph=self.sess.graph)\n",
        "        \n",
        "    def update_policy(self):\n",
        "        # epsilon을 왜 업데이트(혹은 decaying) 해주지??\n",
        "        if self.epsilon > self.epsilon_min: # Update epsilon\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    def update_target(self): # Update parameters\n",
        "        self.sess.run(self.update_ops)\n",
        "    \n",
        "    def get_prediction_old(self, obs): \n",
        "        # Get Q value \n",
        "        # from target network\n",
        "        q_value_old, summary = self.sess.run([self.q_predict_old,self.merge_q],feed_dict={self.obs_ph:obs}) \n",
        "        \n",
        "        # Summary Q value\n",
        "        self.merge_q_step += 1\n",
        "        self.summary_writer.add_summary(summary,self.merge_q_step)       \n",
        "        \n",
        "        return q_value_old\n",
        "    \n",
        "    def get_prediction(self, obs): \n",
        "        # Get Q value \n",
        "        # from prediction network\n",
        "        q_value, summary = self.sess.run([self.q_predict,self.merge_q],feed_dict={self.obs_ph:obs}) \n",
        "        \n",
        "        # Summary Q value\n",
        "        self.merge_q_step += 1\n",
        "        self.summary_writer.add_summary(summary,self.merge_q_step)       \n",
        "        \n",
        "        return q_value\n",
        "    \n",
        "    def get_action(self, obs): # Epsilon Greedy policy\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.n_action)\n",
        "        else:\n",
        "            q_value = self.get_prediction([obs])\n",
        "            return np.argmax(q_value[0])\n",
        "\n",
        "    def add_experience(self, obs, action, reward, next_obs, done): # Add experience to memory\n",
        "        self.memory.append((obs, action, reward, next_obs, done))\n",
        "\n",
        "    def train_model(self):\n",
        "        loss = np.nan\n",
        "        n_entries = len(self.memory)\n",
        "            \n",
        "        if n_entries > self.train_start: # Start training when the number of experience is greater than batch size\n",
        "            \n",
        "            # Randomly sample batch\n",
        "            mini_batch = random.sample(self.memory, self.batch_size)\n",
        "            \n",
        "            observations = np.zeros((self.batch_size, self.obs_dim))\n",
        "            next_observations = np.zeros((self.batch_size, self.obs_dim))\n",
        "            actions, rewards, dones = [], [], []\n",
        "\n",
        "            for i in range(self.batch_size):\n",
        "                observations[i] = mini_batch[i][0]\n",
        "                actions.append(mini_batch[i][1])\n",
        "                rewards.append(mini_batch[i][2])\n",
        "                next_observations[i] = mini_batch[i][3]\n",
        "                dones.append(mini_batch[i][4])\n",
        "\n",
        "            target = self.get_prediction(observations)\n",
        "            next_q_value = self.get_prediction_old(next_observations)\n",
        "\n",
        "            # BELLMAN UPDATE RULE \n",
        "            for i in range(self.batch_size):\n",
        "                if dones[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else:\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * (np.amax(next_q_value[i]))\n",
        "\n",
        "            loss, _, summary = self.sess.run([self.loss, self.optim, self.merge_loss], \n",
        "                                 feed_dict={self.obs_ph:observations,self.target_ph:target,self.learning_rate_ph:self.learning_rate})                        \n",
        "            \n",
        "            # Summary loss\n",
        "            self.merge_loss_step += 1\n",
        "            self.summary_writer.add_summary(summary, self.merge_loss_step)       \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2hkz7VltRgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Environment and Agent"
      ]
    },
    {
      "metadata": {
        "id": "VItBfACgtRgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "f8e0c500-2a45-41e4-867e-b84fced3b00d"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "obs_space = env.observation_space\n",
        "print('Observation space')\n",
        "print(type(obs_space))\n",
        "print(obs_space.shape)\n",
        "print(\"Dimension:{}\".format(obs_space.shape[0]))\n",
        "print(\"High: {}\".format(obs_space.high))\n",
        "print(\"Low: {}\".format(obs_space.low))\n",
        "print()\n",
        "\n",
        "act_space = env.action_space\n",
        "print('Action space')\n",
        "print(type(act_space))\n",
        "print(\"Total {} actions\".format(act_space.n))\n",
        "print()\n",
        "\n",
        "env.seed(seed)\n",
        "max_t = env.spec.max_episode_steps\n",
        "agent = DQNAgent(env.observation_space.high.shape[0],env.action_space.n)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Observation space\n",
            "<class 'gym.spaces.box.Box'>\n",
            "(4,)\n",
            "Dimension:4\n",
            "High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
            "Low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
            "\n",
            "Action space\n",
            "<class 'gym.spaces.discrete.Discrete'>\n",
            "Total 2 actions\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BmceOm_MtRgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train"
      ]
    },
    {
      "metadata": {
        "id": "Mcr0m9AAtRgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4f386c26-c0d6-4c82-81d1-5a7e67c7a922"
      },
      "cell_type": "code",
      "source": [
        "avg_return_list = deque(maxlen=10)\n",
        "avg_loss_list = deque(maxlen=10)\n",
        "nepisodes = 1500\n",
        "for i in range(nepisodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    total_loss = 0\n",
        "    for t in range(max_t):\n",
        "        # Get transition\n",
        "        action = agent.get_action(obs)\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Add experience\n",
        "        agent.add_experience(obs,action,reward,next_obs,done)\n",
        "        \n",
        "        # Online update perdiction network parameter\n",
        "        loss = agent.train_model()\n",
        "        agent.update_policy()\n",
        "                \n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        total_loss += loss\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # Update target network parameter\n",
        "    agent.update_target()\n",
        "    avg_return_list.append(total_reward)\n",
        "    avg_loss_list.append(total_loss)\n",
        "    \n",
        "    if (np.mean(avg_return_list) > 490): # Threshold return to success cartpole\n",
        "        print('[{}/{}] loss : {:.3f}, return : {:.3f}, eps : {:.3f}'.format(i,nepisodes, np.mean(avg_loss_list), np.mean(avg_return_list), agent.epsilon))\n",
        "        print('The problem is solved with {} episodes'.format(i))\n",
        "        break\n",
        "    \n",
        "    if (i%100)==0:\n",
        "        print('[{}/{}] loss : {:.3f}, return : {:.3f}, eps : {:.3f}'.format(i,nepisodes, np.mean(avg_loss_list), np.mean(avg_return_list), agent.epsilon))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/1500] loss : nan, return : 11.000, eps : 0.989\n",
            "[100/1500] loss : 220.970, return : 56.100, eps : 0.098\n",
            "[200/1500] loss : 43.845, return : 109.600, eps : 0.010\n",
            "[299/1500] loss : 3473.439, return : 500.000, eps : 0.010\n",
            "The problem is solved with 299 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LTBwYzRqtRga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test"
      ]
    },
    {
      "metadata": {
        "id": "TunU_lIZysBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "dcf5f8f4-fdc1-4ad5-a136-5282ff879df4"
      },
      "cell_type": "code",
      "source": [
        "!sudo pip3 install PyOpenGL \n",
        "!sudo pip3 install piglet \n",
        "!sudo pip3 install pyglet"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (18.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.32.3)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Br269UlutRgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "outputId": "bd5a7b1f-5192-4fd9-848d-2718da21f1ea"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "obs = env.reset()\n",
        "total_reward = 0\n",
        "frames = []\n",
        "for t in range(10000):\n",
        "    # Render into buffer. \n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    action = agent.get_action(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "print('Total Reward : %.2f'%total_reward)\n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-db712b46390a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Render into buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zrpav58r63mA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "what the...."
      ]
    }
  ]
}